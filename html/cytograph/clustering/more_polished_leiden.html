<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>cytograph.clustering.more_polished_leiden API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cytograph.clustering.more_polished_leiden</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
from typing import Tuple
import leidenalg as la
import igraph
import numpy as np
from cytograph import requires, creates, Algorithm
import shoji
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.calibration import CalibratedClassifierCV
from sklearn.linear_model import SGDClassifier


class MorePolishedLeiden(Algorithm):
        &#34;&#34;&#34;
        Leiden clustering with some post-processing
        &#34;&#34;&#34;
        def __init__(self, resolution: float = 1.0, method: str = &#34;modularity&#34;, max_size: int = 0, min_size: int = 10, **kwargs) -&gt; None:
                &#34;&#34;&#34;
                Find clusters on the manifold using the Leiden algorithm, then polish clusters on the embedding.

                Args:
                        resolution: The resolution parameter (typically 0.01 - 1; default: 1)
                        method:     The partitioning method (&#34;modularity&#34;, &#34;cpm&#34;, &#34;surprise&#34;, &#34;rb&#34;, &#34;rber&#34;, or &#34;significance&#34;; default: &#34;modularity&#34;)
                        max_size:   The maximum size of clusters (default: 0, i.e. no limit)
                        min_size:   The minimum size of clusters (default: 10)

                Remarks:
                        The default method, &#34;modularity&#34;, is equivalent to Louvain clustering but the Leiden algorithm is faster
                        and can yield better clusters. The method &#34;surprise&#34; can yield clusters similar to Amos Tanay&#39;s Metacell
                        algorithm (i.e. a tiling of the manifold with clusters of similar size), especially if `max_size` is used
                        to cap the cluster size.

                        The polishing step consists of two phases. First, the shape of each cluster on the embedding (e.g. TSNE) is
                        considered using Iglewicz-Hoaglin outlier detection, and clusters with too many outliers are re-clustered
                        using DBSCAN on the embedding. This step is identical to standard PolishedLeiden. 
                        
                        Second, a classifier is trained on all clusters larger than `min_size`, and
                        all cells are reassigned to their maximum probability cluster. This removes all small clusters and outliers.
                        Note that this can result in clusters larger than `max_size`. The probability of the cluster label for
                        each cell is returned, and can be used to identify outliers or intermediate cell states.

                        The resolution parameter has no effect on the default method, &#34;modularity&#34;.
                &#34;&#34;&#34;
                super().__init__(**kwargs)
                self.resolution = resolution
                ptypes = {
                        &#34;modularity&#34;: la.ModularityVertexPartition,
                        &#34;cpm&#34;: la.CPMVertexPartition,
                        &#34;surprise&#34;: la.SurpriseVertexPartition,
                        &#34;rb&#34;: la.RBConfigurationVertexPartition,
                        &#34;rber&#34;: la.RBERVertexPartition,
                        &#34;significance&#34;: la.SignificanceVertexPartition
                }
                if method.lower() in ptypes:
                        self.method = ptypes[method.lower()]
                else:
                        raise ValueError(f&#34;Invalid partition method &#39;{method}&#39;&#34;)
                self.max_size = max_size
                self.min_size = min_size

        def _is_outlier(self, points: np.ndarray, thresh: float = 3.5) -&gt; np.ndarray:
                &#34;&#34;&#34;
                Returns a boolean array with True if points are outliers and False
                otherwise.

                Parameters:
                -----------
                        points : An numobservations by numdimensions array of observations
                        thresh : The modified z-score to use as a threshold. Observations with
                                a modified z-score (based on the median absolute deviation) greater
                                than this value will be classified as outliers.

                Returns:
                --------
                        mask : A numobservations-length boolean array.

                References:
                ----------
                        Boris Iglewicz and David Hoaglin (1993), &#34;Volume 16: How to Detect and
                        Handle Outliers&#34;, The ASQC Basic References in Quality Control:
                        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor.
                &#34;&#34;&#34;
                if len(points.shape) == 1:
                        points = points[:, None]
                median = np.median(points, axis=0)
                diff = np.sum((points - median)**2, axis=-1)
                diff = np.sqrt(diff)
                med_abs_deviation = np.median(diff)

                modified_z_score = 0.6745 * diff / med_abs_deviation

                return modified_z_score &gt; thresh

        def _break_cluster(self, embedding: np.ndarray) -&gt; np.ndarray:
                &#34;&#34;&#34;
                If needed, split the cluster by density clustering on the embedding

                Returns:
                        An array of cluster labels (all zeros if cluster wasn&#39;t split)
                        Note: the returned array may contain -1 for outliers
                &#34;&#34;&#34;
                # Find outliers in either dimension using Grubbs test
                xy = PCA().fit_transform(embedding)
                x = xy[:, 0]
                y = xy[:, 1]
                # Standardize x and y (not sure if this is really necessary)
                x = (x - x.mean()) / x.std()
                y = (y - y.mean()) / y.std()
                xy = np.vstack([x, y]).transpose()

                outliers = np.zeros(embedding.shape[0], dtype=&#39;bool&#39;)
                for _ in range(5):
                        outliers[~outliers] = self._is_outlier(x[~outliers])
                        outliers[~outliers] = self._is_outlier(y[~outliers])

                # See if the cluster is very dispersed
                min_pts = min(50, min(x.shape[0] - 1, max(5, round(0.1 * x.shape[0]))))
                nn = NearestNeighbors(n_neighbors=min_pts, algorithm=&#34;ball_tree&#34;, n_jobs=4)
                nn.fit(xy)
                knn = nn.kneighbors_graph(mode=&#39;distance&#39;)
                k_radius = knn.max(axis=1).toarray()
                epsilon = np.percentile(k_radius, 70)
                # Not too many outliers, and not too dispersed
                if outliers.sum() &lt;= 3 and (np.sqrt(x**2 + y**2) &lt; epsilon).sum() &gt;= min_pts * 0.5:
                        return np.zeros(embedding.shape[0], dtype=&#39;int&#39;)

                # Too many outliers, or too dispersed
                clusterer = DBSCAN(eps=epsilon, min_samples=round(min_pts * 0.5))
                labels = clusterer.fit_predict(xy)

                # Assign each outlier to the same cluster as the nearest non-outlier
                if (labels == -1).sum() &gt; 0:
                        nn = NearestNeighbors(n_neighbors=50, algorithm=&#34;ball_tree&#34;)
                        nn.fit(xy[labels &gt;= 0])
                        nearest = nn.kneighbors(xy[labels == -1], n_neighbors=1, return_distance=False)
                        labels[labels == -1] = labels[labels &gt;= 0][nearest.flat[:]]
                return labels

        @requires(&#34;Embedding&#34;, &#34;float32&#34;, (&#34;cells&#34;, 2))
        @requires(&#34;Factors&#34;, &#34;float32&#34;, (&#34;cells&#34;, None))
        @requires(&#34;ManifoldIndices&#34;, &#34;uint32&#34;, (None, 2))
        @requires(&#34;ManifoldWeights&#34;, &#34;float32&#34;, (None))
        @creates(&#34;Clusters&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersSecondary&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersSecondaryProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
        def fit(self, ws: shoji.WorkspaceManager, save: bool = False) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
                &#34;&#34;&#34;
                Given a sparse adjacency matrix, perform Leiden clustering

                Args:
                        ws              The shoji Workspace

                Returns:
                        labels: The cluster labels
                &#34;&#34;&#34;
                n_cells = ws.cells.length
                logging.info(&#34; MorePolishedLeiden: Loading graph data&#34;)
                rc = self.ManifoldIndices[:]
                logging.info(&#34; MorePolishedLeiden: Constructing the graph&#34;)
                weights = self.ManifoldWeights[:]
                g = igraph.Graph(ws.cells.length, list(zip(rc[:, 0].T.tolist(), rc[:, 1].T.tolist())), directed=False, edge_attrs={&#39;weight&#39;: weights})
                logging.info(&#34; MorePolishedLeiden: Optimizing the graph partitioning&#34;)
                if self.resolution != 1:
                        labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, resolution_parameter=self.resolution, n_iterations=-1).membership)
                else:
                        labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, n_iterations=-1).membership)
                logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} initial clusters&#34;)

                # Break clusters based on the embedding
                logging.info(&#34; MorePolishedLeiden: Breaking clusters based on the embedding&#34;)
                xy = self.Embedding[:]
                # Only break clusters that are at least twice as large as the minimum size (note: labels are sorted by cluster size)
                max_label = np.where(np.bincount(labels) &lt; self.min_size * 2)[0][0]
                next_label = 0
                labels2 = np.copy(labels)
                for lbl in range(max_label):
                        cluster = labels == lbl
                        if cluster.sum() &lt; self.min_size:
                                continue
                        adjusted = self._break_cluster(xy[cluster, :])
                        new_labels = np.copy(adjusted)
                        for i in range(np.max(adjusted) + 1):
                                new_labels[adjusted == i] = i + next_label
                        next_label = next_label + np.max(adjusted) + 1
                        labels2[cluster] = new_labels
                labels = labels2
                logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} clusters after breaking clusters on the embedding&#34;)

                # Assign each orphan cell to the same cluster as the nearest non-orphan
                logging.info(f&#34; MorePolishedLeiden: Removing clusters with less than {self.min_size} cells&#34;)
                too_small = np.isin(labels, np.where(np.bincount(labels) &lt; self.min_size)[0])
                # Relabel, in case some labels are missing
                labels_not_too_small = LabelEncoder().fit_transform(labels[~too_small])
                n_large_clusters = np.unique(labels_not_too_small).shape[0]
                logging.info(f&#34; MorePolishedLeiden: {too_small.sum()} cells lost their cluster labels ({int(too_small.sum() / n_cells * 100)}%)&#34;)

                logging.info(f&#34; MorePolishedLeiden: Reclassifying all cells to the remaining {n_large_clusters} clusters&#34;)
                factors = self.Factors[:]
                sgdc = SGDClassifier(loss=&#34;hinge&#34;, penalty=&#34;l2&#34;, class_weight=&#39;balanced&#39;, tol=0.01, n_jobs=-1)
                classifier = make_pipeline(StandardScaler(), CalibratedClassifierCV(sgdc))
                classifier.fit(factors[~too_small, :], labels_not_too_small)

                # Avoid materializing a whole probability matrix (n_cells, n_clusters), which might be too large
                ix = 0
                BATCH_SIZE = 10_000
                predicted = np.zeros(n_cells, dtype=&#34;uint32&#34;)
                secondary = np.zeros(n_cells, dtype=&#34;uint32&#34;)
                predicted_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
                secondary_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
                while ix &lt; n_cells:
                        probs = classifier.predict_proba(factors[ix: ix + BATCH_SIZE])
                        ordered = probs.argsort(axis=1)
                        predicted[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -1]]
                        secondary[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -2]]
                        predicted_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -1]]
                        secondary_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -2]]
                        ix += BATCH_SIZE
                
                labels[too_small] = predicted[too_small]  # New labels for the too_small clusters
                labels[~too_small] = labels_not_too_small  # Keep the labels for the not too small clusters, to avoid holes in the label sequence
                assert len(np.unique(labels)) == labels.max() + 1, &#34;Missing cluster labels due to reclassification&#34;

                accuracy = (predicted[~too_small] == labels[~too_small]).sum() / (~too_small).sum()
                logging.info(f&#34; MorePolishedLeiden: {int(accuracy * 100)}% classification accuracy on non-orphan cells&#34;)
                return labels, secondary, predicted_proba, secondary_proba</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cytograph.clustering.more_polished_leiden.MorePolishedLeiden"><code class="flex name class">
<span>class <span class="ident">MorePolishedLeiden</span></span>
<span>(</span><span>resolution: float = 1.0, method: str = 'modularity', max_size: int = 0, min_size: int = 10, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Leiden clustering with some post-processing</p>
<p>Find clusters on the manifold using the Leiden algorithm, then polish clusters on the embedding.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>The resolution parameter (typically 0.01 - 1; default: 1)</dd>
<dt><strong><code>method</code></strong></dt>
<dd>
<p>The partitioning method ("modularity", "cpm", "surprise", "rb", "rber", or "significance"; default: "modularity")</p>
</dd>
<dt><strong><code>max_size</code></strong></dt>
<dd>The maximum size of clusters (default: 0, i.e. no limit)</dd>
<dt><strong><code>min_size</code></strong></dt>
<dd>The minimum size of clusters (default: 10)</dd>
</dl>
<h2 id="remarks">Remarks</h2>
<p>The default method, "modularity", is equivalent to Louvain clustering but the Leiden algorithm is faster
and can yield better clusters. The method "surprise" can yield clusters similar to Amos Tanay's Metacell
algorithm (i.e. a tiling of the manifold with clusters of similar size), especially if <code>max_size</code> is used
to cap the cluster size.</p>
<p>The polishing step consists of two phases. First, the shape of each cluster on the embedding (e.g. TSNE) is
considered using Iglewicz-Hoaglin outlier detection, and clusters with too many outliers are re-clustered
using DBSCAN on the embedding. This step is identical to standard PolishedLeiden. </p>
<p>Second, a classifier is trained on all clusters larger than <code>min_size</code>, and
all cells are reassigned to their maximum probability cluster. This removes all small clusters and outliers.
Note that this can result in clusters larger than <code>max_size</code>. The probability of the cluster label for
each cell is returned, and can be used to identify outliers or intermediate cell states.</p>
<p>The resolution parameter has no effect on the default method, "modularity".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MorePolishedLeiden(Algorithm):
        &#34;&#34;&#34;
        Leiden clustering with some post-processing
        &#34;&#34;&#34;
        def __init__(self, resolution: float = 1.0, method: str = &#34;modularity&#34;, max_size: int = 0, min_size: int = 10, **kwargs) -&gt; None:
                &#34;&#34;&#34;
                Find clusters on the manifold using the Leiden algorithm, then polish clusters on the embedding.

                Args:
                        resolution: The resolution parameter (typically 0.01 - 1; default: 1)
                        method:     The partitioning method (&#34;modularity&#34;, &#34;cpm&#34;, &#34;surprise&#34;, &#34;rb&#34;, &#34;rber&#34;, or &#34;significance&#34;; default: &#34;modularity&#34;)
                        max_size:   The maximum size of clusters (default: 0, i.e. no limit)
                        min_size:   The minimum size of clusters (default: 10)

                Remarks:
                        The default method, &#34;modularity&#34;, is equivalent to Louvain clustering but the Leiden algorithm is faster
                        and can yield better clusters. The method &#34;surprise&#34; can yield clusters similar to Amos Tanay&#39;s Metacell
                        algorithm (i.e. a tiling of the manifold with clusters of similar size), especially if `max_size` is used
                        to cap the cluster size.

                        The polishing step consists of two phases. First, the shape of each cluster on the embedding (e.g. TSNE) is
                        considered using Iglewicz-Hoaglin outlier detection, and clusters with too many outliers are re-clustered
                        using DBSCAN on the embedding. This step is identical to standard PolishedLeiden. 
                        
                        Second, a classifier is trained on all clusters larger than `min_size`, and
                        all cells are reassigned to their maximum probability cluster. This removes all small clusters and outliers.
                        Note that this can result in clusters larger than `max_size`. The probability of the cluster label for
                        each cell is returned, and can be used to identify outliers or intermediate cell states.

                        The resolution parameter has no effect on the default method, &#34;modularity&#34;.
                &#34;&#34;&#34;
                super().__init__(**kwargs)
                self.resolution = resolution
                ptypes = {
                        &#34;modularity&#34;: la.ModularityVertexPartition,
                        &#34;cpm&#34;: la.CPMVertexPartition,
                        &#34;surprise&#34;: la.SurpriseVertexPartition,
                        &#34;rb&#34;: la.RBConfigurationVertexPartition,
                        &#34;rber&#34;: la.RBERVertexPartition,
                        &#34;significance&#34;: la.SignificanceVertexPartition
                }
                if method.lower() in ptypes:
                        self.method = ptypes[method.lower()]
                else:
                        raise ValueError(f&#34;Invalid partition method &#39;{method}&#39;&#34;)
                self.max_size = max_size
                self.min_size = min_size

        def _is_outlier(self, points: np.ndarray, thresh: float = 3.5) -&gt; np.ndarray:
                &#34;&#34;&#34;
                Returns a boolean array with True if points are outliers and False
                otherwise.

                Parameters:
                -----------
                        points : An numobservations by numdimensions array of observations
                        thresh : The modified z-score to use as a threshold. Observations with
                                a modified z-score (based on the median absolute deviation) greater
                                than this value will be classified as outliers.

                Returns:
                --------
                        mask : A numobservations-length boolean array.

                References:
                ----------
                        Boris Iglewicz and David Hoaglin (1993), &#34;Volume 16: How to Detect and
                        Handle Outliers&#34;, The ASQC Basic References in Quality Control:
                        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor.
                &#34;&#34;&#34;
                if len(points.shape) == 1:
                        points = points[:, None]
                median = np.median(points, axis=0)
                diff = np.sum((points - median)**2, axis=-1)
                diff = np.sqrt(diff)
                med_abs_deviation = np.median(diff)

                modified_z_score = 0.6745 * diff / med_abs_deviation

                return modified_z_score &gt; thresh

        def _break_cluster(self, embedding: np.ndarray) -&gt; np.ndarray:
                &#34;&#34;&#34;
                If needed, split the cluster by density clustering on the embedding

                Returns:
                        An array of cluster labels (all zeros if cluster wasn&#39;t split)
                        Note: the returned array may contain -1 for outliers
                &#34;&#34;&#34;
                # Find outliers in either dimension using Grubbs test
                xy = PCA().fit_transform(embedding)
                x = xy[:, 0]
                y = xy[:, 1]
                # Standardize x and y (not sure if this is really necessary)
                x = (x - x.mean()) / x.std()
                y = (y - y.mean()) / y.std()
                xy = np.vstack([x, y]).transpose()

                outliers = np.zeros(embedding.shape[0], dtype=&#39;bool&#39;)
                for _ in range(5):
                        outliers[~outliers] = self._is_outlier(x[~outliers])
                        outliers[~outliers] = self._is_outlier(y[~outliers])

                # See if the cluster is very dispersed
                min_pts = min(50, min(x.shape[0] - 1, max(5, round(0.1 * x.shape[0]))))
                nn = NearestNeighbors(n_neighbors=min_pts, algorithm=&#34;ball_tree&#34;, n_jobs=4)
                nn.fit(xy)
                knn = nn.kneighbors_graph(mode=&#39;distance&#39;)
                k_radius = knn.max(axis=1).toarray()
                epsilon = np.percentile(k_radius, 70)
                # Not too many outliers, and not too dispersed
                if outliers.sum() &lt;= 3 and (np.sqrt(x**2 + y**2) &lt; epsilon).sum() &gt;= min_pts * 0.5:
                        return np.zeros(embedding.shape[0], dtype=&#39;int&#39;)

                # Too many outliers, or too dispersed
                clusterer = DBSCAN(eps=epsilon, min_samples=round(min_pts * 0.5))
                labels = clusterer.fit_predict(xy)

                # Assign each outlier to the same cluster as the nearest non-outlier
                if (labels == -1).sum() &gt; 0:
                        nn = NearestNeighbors(n_neighbors=50, algorithm=&#34;ball_tree&#34;)
                        nn.fit(xy[labels &gt;= 0])
                        nearest = nn.kneighbors(xy[labels == -1], n_neighbors=1, return_distance=False)
                        labels[labels == -1] = labels[labels &gt;= 0][nearest.flat[:]]
                return labels

        @requires(&#34;Embedding&#34;, &#34;float32&#34;, (&#34;cells&#34;, 2))
        @requires(&#34;Factors&#34;, &#34;float32&#34;, (&#34;cells&#34;, None))
        @requires(&#34;ManifoldIndices&#34;, &#34;uint32&#34;, (None, 2))
        @requires(&#34;ManifoldWeights&#34;, &#34;float32&#34;, (None))
        @creates(&#34;Clusters&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersSecondary&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
        @creates(&#34;ClustersSecondaryProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
        def fit(self, ws: shoji.WorkspaceManager, save: bool = False) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
                &#34;&#34;&#34;
                Given a sparse adjacency matrix, perform Leiden clustering

                Args:
                        ws              The shoji Workspace

                Returns:
                        labels: The cluster labels
                &#34;&#34;&#34;
                n_cells = ws.cells.length
                logging.info(&#34; MorePolishedLeiden: Loading graph data&#34;)
                rc = self.ManifoldIndices[:]
                logging.info(&#34; MorePolishedLeiden: Constructing the graph&#34;)
                weights = self.ManifoldWeights[:]
                g = igraph.Graph(ws.cells.length, list(zip(rc[:, 0].T.tolist(), rc[:, 1].T.tolist())), directed=False, edge_attrs={&#39;weight&#39;: weights})
                logging.info(&#34; MorePolishedLeiden: Optimizing the graph partitioning&#34;)
                if self.resolution != 1:
                        labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, resolution_parameter=self.resolution, n_iterations=-1).membership)
                else:
                        labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, n_iterations=-1).membership)
                logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} initial clusters&#34;)

                # Break clusters based on the embedding
                logging.info(&#34; MorePolishedLeiden: Breaking clusters based on the embedding&#34;)
                xy = self.Embedding[:]
                # Only break clusters that are at least twice as large as the minimum size (note: labels are sorted by cluster size)
                max_label = np.where(np.bincount(labels) &lt; self.min_size * 2)[0][0]
                next_label = 0
                labels2 = np.copy(labels)
                for lbl in range(max_label):
                        cluster = labels == lbl
                        if cluster.sum() &lt; self.min_size:
                                continue
                        adjusted = self._break_cluster(xy[cluster, :])
                        new_labels = np.copy(adjusted)
                        for i in range(np.max(adjusted) + 1):
                                new_labels[adjusted == i] = i + next_label
                        next_label = next_label + np.max(adjusted) + 1
                        labels2[cluster] = new_labels
                labels = labels2
                logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} clusters after breaking clusters on the embedding&#34;)

                # Assign each orphan cell to the same cluster as the nearest non-orphan
                logging.info(f&#34; MorePolishedLeiden: Removing clusters with less than {self.min_size} cells&#34;)
                too_small = np.isin(labels, np.where(np.bincount(labels) &lt; self.min_size)[0])
                # Relabel, in case some labels are missing
                labels_not_too_small = LabelEncoder().fit_transform(labels[~too_small])
                n_large_clusters = np.unique(labels_not_too_small).shape[0]
                logging.info(f&#34; MorePolishedLeiden: {too_small.sum()} cells lost their cluster labels ({int(too_small.sum() / n_cells * 100)}%)&#34;)

                logging.info(f&#34; MorePolishedLeiden: Reclassifying all cells to the remaining {n_large_clusters} clusters&#34;)
                factors = self.Factors[:]
                sgdc = SGDClassifier(loss=&#34;hinge&#34;, penalty=&#34;l2&#34;, class_weight=&#39;balanced&#39;, tol=0.01, n_jobs=-1)
                classifier = make_pipeline(StandardScaler(), CalibratedClassifierCV(sgdc))
                classifier.fit(factors[~too_small, :], labels_not_too_small)

                # Avoid materializing a whole probability matrix (n_cells, n_clusters), which might be too large
                ix = 0
                BATCH_SIZE = 10_000
                predicted = np.zeros(n_cells, dtype=&#34;uint32&#34;)
                secondary = np.zeros(n_cells, dtype=&#34;uint32&#34;)
                predicted_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
                secondary_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
                while ix &lt; n_cells:
                        probs = classifier.predict_proba(factors[ix: ix + BATCH_SIZE])
                        ordered = probs.argsort(axis=1)
                        predicted[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -1]]
                        secondary[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -2]]
                        predicted_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -1]]
                        secondary_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -2]]
                        ix += BATCH_SIZE
                
                labels[too_small] = predicted[too_small]  # New labels for the too_small clusters
                labels[~too_small] = labels_not_too_small  # Keep the labels for the not too small clusters, to avoid holes in the label sequence
                assert len(np.unique(labels)) == labels.max() + 1, &#34;Missing cluster labels due to reclassification&#34;

                accuracy = (predicted[~too_small] == labels[~too_small]).sum() / (~too_small).sum()
                logging.info(f&#34; MorePolishedLeiden: {int(accuracy * 100)}% classification accuracy on non-orphan cells&#34;)
                return labels, secondary, predicted_proba, secondary_proba</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="cytograph.algorithm.Algorithm" href="../algorithm.html#cytograph.algorithm.Algorithm">Algorithm</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="cytograph.clustering.more_polished_leiden.MorePolishedLeiden.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, ws: shoji.workspace.WorkspaceManager, save: bool = False) ‑> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Given a sparse adjacency matrix, perform Leiden clustering</p>
<h2 id="args">Args</h2>
<p>ws
The shoji Workspace</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>labels</code></dt>
<dd>The cluster labels</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@requires(&#34;Embedding&#34;, &#34;float32&#34;, (&#34;cells&#34;, 2))
@requires(&#34;Factors&#34;, &#34;float32&#34;, (&#34;cells&#34;, None))
@requires(&#34;ManifoldIndices&#34;, &#34;uint32&#34;, (None, 2))
@requires(&#34;ManifoldWeights&#34;, &#34;float32&#34;, (None))
@creates(&#34;Clusters&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
@creates(&#34;ClustersSecondary&#34;, &#34;uint32&#34;, (&#34;cells&#34;,))
@creates(&#34;ClustersProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
@creates(&#34;ClustersSecondaryProbability&#34;, &#34;float32&#34;, (&#34;cells&#34;,))
def fit(self, ws: shoji.WorkspaceManager, save: bool = False) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        Given a sparse adjacency matrix, perform Leiden clustering

        Args:
                ws              The shoji Workspace

        Returns:
                labels: The cluster labels
        &#34;&#34;&#34;
        n_cells = ws.cells.length
        logging.info(&#34; MorePolishedLeiden: Loading graph data&#34;)
        rc = self.ManifoldIndices[:]
        logging.info(&#34; MorePolishedLeiden: Constructing the graph&#34;)
        weights = self.ManifoldWeights[:]
        g = igraph.Graph(ws.cells.length, list(zip(rc[:, 0].T.tolist(), rc[:, 1].T.tolist())), directed=False, edge_attrs={&#39;weight&#39;: weights})
        logging.info(&#34; MorePolishedLeiden: Optimizing the graph partitioning&#34;)
        if self.resolution != 1:
                labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, resolution_parameter=self.resolution, n_iterations=-1).membership)
        else:
                labels = np.array(la.find_partition(g, self.method, weights=weights, max_comm_size=self.max_size, n_iterations=-1).membership)
        logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} initial clusters&#34;)

        # Break clusters based on the embedding
        logging.info(&#34; MorePolishedLeiden: Breaking clusters based on the embedding&#34;)
        xy = self.Embedding[:]
        # Only break clusters that are at least twice as large as the minimum size (note: labels are sorted by cluster size)
        max_label = np.where(np.bincount(labels) &lt; self.min_size * 2)[0][0]
        next_label = 0
        labels2 = np.copy(labels)
        for lbl in range(max_label):
                cluster = labels == lbl
                if cluster.sum() &lt; self.min_size:
                        continue
                adjusted = self._break_cluster(xy[cluster, :])
                new_labels = np.copy(adjusted)
                for i in range(np.max(adjusted) + 1):
                        new_labels[adjusted == i] = i + next_label
                next_label = next_label + np.max(adjusted) + 1
                labels2[cluster] = new_labels
        labels = labels2
        logging.info(f&#34; MorePolishedLeiden: Found {labels.max() + 1} clusters after breaking clusters on the embedding&#34;)

        # Assign each orphan cell to the same cluster as the nearest non-orphan
        logging.info(f&#34; MorePolishedLeiden: Removing clusters with less than {self.min_size} cells&#34;)
        too_small = np.isin(labels, np.where(np.bincount(labels) &lt; self.min_size)[0])
        # Relabel, in case some labels are missing
        labels_not_too_small = LabelEncoder().fit_transform(labels[~too_small])
        n_large_clusters = np.unique(labels_not_too_small).shape[0]
        logging.info(f&#34; MorePolishedLeiden: {too_small.sum()} cells lost their cluster labels ({int(too_small.sum() / n_cells * 100)}%)&#34;)

        logging.info(f&#34; MorePolishedLeiden: Reclassifying all cells to the remaining {n_large_clusters} clusters&#34;)
        factors = self.Factors[:]
        sgdc = SGDClassifier(loss=&#34;hinge&#34;, penalty=&#34;l2&#34;, class_weight=&#39;balanced&#39;, tol=0.01, n_jobs=-1)
        classifier = make_pipeline(StandardScaler(), CalibratedClassifierCV(sgdc))
        classifier.fit(factors[~too_small, :], labels_not_too_small)

        # Avoid materializing a whole probability matrix (n_cells, n_clusters), which might be too large
        ix = 0
        BATCH_SIZE = 10_000
        predicted = np.zeros(n_cells, dtype=&#34;uint32&#34;)
        secondary = np.zeros(n_cells, dtype=&#34;uint32&#34;)
        predicted_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
        secondary_proba = np.zeros(n_cells, dtype=&#34;float32&#34;)
        while ix &lt; n_cells:
                probs = classifier.predict_proba(factors[ix: ix + BATCH_SIZE])
                ordered = probs.argsort(axis=1)
                predicted[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -1]]
                secondary[ix: ix + BATCH_SIZE] = classifier.classes_[ordered[:, -2]]
                predicted_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -1]]
                secondary_proba[ix: ix + BATCH_SIZE] = probs[np.arange(len(ordered)), ordered[:, -2]]
                ix += BATCH_SIZE
        
        labels[too_small] = predicted[too_small]  # New labels for the too_small clusters
        labels[~too_small] = labels_not_too_small  # Keep the labels for the not too small clusters, to avoid holes in the label sequence
        assert len(np.unique(labels)) == labels.max() + 1, &#34;Missing cluster labels due to reclassification&#34;

        accuracy = (predicted[~too_small] == labels[~too_small]).sum() / (~too_small).sum()
        logging.info(f&#34; MorePolishedLeiden: {int(accuracy * 100)}% classification accuracy on non-orphan cells&#34;)
        return labels, secondary, predicted_proba, secondary_proba</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cytograph.clustering" href="index.html">cytograph.clustering</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cytograph.clustering.more_polished_leiden.MorePolishedLeiden" href="#cytograph.clustering.more_polished_leiden.MorePolishedLeiden">MorePolishedLeiden</a></code></h4>
<ul class="">
<li><code><a title="cytograph.clustering.more_polished_leiden.MorePolishedLeiden.fit" href="#cytograph.clustering.more_polished_leiden.MorePolishedLeiden.fit">fit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>